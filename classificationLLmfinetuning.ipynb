{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2e43bb",
   "metadata": {
    "tags": [
     "DATA PREPARATION"
    ],
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "STAGE 1\n",
    "# DATA PREPARATION\n",
    "\n",
    "DOWNLOAD DATASET\n",
    "DATA PREPROCESSING\n",
    "CREATE DATALOADERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa6e62",
   "metadata": {},
   "source": [
    "STAGE 2\n",
    "# MODEL SETUP\n",
    "INITIALIZE THE MODEL\n",
    "LOAD PRETRAINED MODEL WEIGHTS OF GPT 2\n",
    "MODIFY THE FINAL OUTPUT LAYERS OF MODEL FOR FINETUNING\n",
    "IMPLEMENT THE EVALUATION UTILITIES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50b163",
   "metadata": {},
   "source": [
    "STAGE 3\n",
    "# MODEL FINETUNING AND USAGE\n",
    "FINETUNE MODEL\n",
    "EVALUATE THE MODEL >ACCURACY , LOSS .\n",
    "TEST THE MODEL ON NEW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f188a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8dae4f",
   "metadata": {},
   "source": [
    "# stage 1 begning data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a897e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16a6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0  label                                               text\n",
       "1    ham  Go until jurong point, crazy.. Available only ...\n",
       "2    ham                      Ok lar... Joking wif u oni...\n",
       "3   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "4    ham  U dun say so early hor... U c already then say..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./original_dataset/SMSSpamCollection.csv\",sep= \",\", header= None, names=[\"Label\", \"Text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8d6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The description of the dataset \n",
      "        Label                    Text\n",
      "count   5573                    5573\n",
      "unique     3                    5170\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      " -------------------------------\n",
      "The shape of the dataset (5573, 2)\n",
      " -------------------------------\n",
      "The value counts of the dataset\n",
      " Label\n",
      "ham      4825\n",
      "spam      747\n",
      "label       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"The description of the dataset \\n\",df.describe())\n",
    "print(\" -------------------------------\")\n",
    "print(\"The shape of the dataset\",df.shape)\n",
    "print(\" -------------------------------\")\n",
    "print(\"The value counts of the dataset\\n\",df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dfde6",
   "metadata": {},
   "source": [
    "we need to create a balanced dataset you can see ham has 4825 samples and spam has 747 samples, its obviosly imbalanced dataset , we will create a new balanced dataset form this . We will take 747 sample from each class and create a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8954e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import random_state\n",
    "def create_balanced_dataset(df):\n",
    "    random_state= 122\n",
    "    #count the frequncy of spam and extract random samples of ham from dataset , such that both ham and spam sample count or you can say frquency is same\n",
    "    num_spam = df[df['Label'] == \"spam\"].shape[0]\n",
    "    ham_instances = df[df['Label'] == \"ham\"].sample(num_spam, random_state=42) #it will extract 747(num_spam) random samples of ham.\n",
    "    spam_instances = df[df['Label'] == \"spam\"]\n",
    "    balanced_df = pd.concat([ham_instances,spam_instances])\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45dd6bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     747\n",
       "spam    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#main\n",
    "balanced_df =  create_balanced_dataset(df)\n",
    "balanced_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c46c73",
   "metadata": {},
   "source": [
    "# saving the balanced dataset into a seprate csv file \n",
    "we will do training and validation , testing and finetuing with the help of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d432bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_df.to_csv(\"./balanced_dataset_prepared/balanced_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0eb7fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     747\n",
       "spam    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bde20",
   "metadata": {},
   "source": [
    "# we have balanced dataframe prepared already so we will use it and do  Label encoding on it. so that our model understand the numbers better.\n",
    "NOTE: we are just making changes on dataframe and not on original balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a140d5",
   "metadata": {},
   "source": [
    "# Label Encoding of Labels['ham','spam']\n",
    "ham -> 0\n",
    "spam -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287b5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['Label'] = balanced_df['Label'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c29804",
   "metadata": {},
   "source": [
    "you can see that we have encoded the labels spam as 1 and ham as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4eefc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    747\n",
      "1    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e5701",
   "metadata": {},
   "source": [
    "# We will split the dataset into train , validation and test set for this there are two strategy we can direclty use the trani_test_split from sklearn and cleverly make the train , validation and test set.\n",
    "# second option we can create custom random split function to create the train , validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dff855",
   "metadata": {},
   "source": [
    "train 70%,\n",
    "\n",
    "validation 10%,\n",
    "\n",
    "test 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbfcdb",
   "metadata": {},
   "source": [
    "creating a random split function which will split the dataset into train , validation and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e7ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_split(df, train_frac , valid_frac):\n",
    "#     df = df.sample(frac =1, random_state=42, reset_index(drop=True))\n",
    "#     train_size = int(len(df) * train_frac)\n",
    "#     valid_size = int(len(df) * valid_frac)\n",
    "#     train_df = df.iloc[:train_size]\n",
    "#     valid_df = df.iloc[train_size:train_size + valid_size]\n",
    "#     test_df = df.iloc[train_size + valid_size:]\n",
    "#     return train_df, valid_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d008d99",
   "metadata": {},
   "source": [
    "# option 2 from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ca10be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40146acf",
   "metadata": {},
   "source": [
    "note train test split can make two splits train and test, we will first create 70% train_df and 30% temp_test_df split, then we will further split on temp_test_df to make \n",
    "10% val_df and 20% test_df. In this way we can get splitting in the ratio 70:10:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d49774",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, temp_test_df = train_test_split(balanced_df,test_size=0.3, random_state=42) #\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=2/3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e24e741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0</td>\n",
       "      <td>Great to hear you are settling well. So what's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't have anybody's number, I still haven't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>0</td>\n",
       "      <td>No. I dont want to hear anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>0</td>\n",
       "      <td>All these nice new shirts and the only thing I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello, my love! How goes that day ? I wish you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "1301      0  Great to hear you are settling well. So what's...\n",
       "2023      0  I don't have anybody's number, I still haven't...\n",
       "5521      0                   No. I dont want to hear anything\n",
       "2695      0  All these nice new shirts and the only thing I...\n",
       "3485      0  Hello, my love! How goes that day ? I wish you..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b33e11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>1</td>\n",
       "      <td>Ur cash-balance is currently 500 pounds - to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>1</td>\n",
       "      <td>For taking part in our mobile survey yesterday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>1</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>1</td>\n",
       "      <td>Congrats 2 mobile 3G Videophones R yours. call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0</td>\n",
       "      <td>Good. Good job. I like entrepreneurs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "5028      1  Ur cash-balance is currently 500 pounds - to m...\n",
       "1127      1  For taking part in our mobile survey yesterday...\n",
       "5286      1  URGENT! You have won a 1 week FREE membership ...\n",
       "2575      1  Congrats 2 mobile 3G Videophones R yours. call...\n",
       "697       0               Good. Good job. I like entrepreneurs"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9284c544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>0</td>\n",
       "      <td>Only 2% students solved this CAT question in '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>1</td>\n",
       "      <td>Free Msg: Ringtone!From: http://tms. widelive....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>1</td>\n",
       "      <td>Santa Calling! Would your little ones like a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>0</td>\n",
       "      <td>Damn, can you make it tonight or do you want t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>0</td>\n",
       "      <td>I am going to sleep. I am tired of travel.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "2934      0  Only 2% students solved this CAT question in '...\n",
       "3863      1  Free Msg: Ringtone!From: http://tms. widelive....\n",
       "5428      1  Santa Calling! Would your little ones like a c...\n",
       "1219      0  Damn, can you make it tonight or do you want t...\n",
       "4827      0         I am going to sleep. I am tired of travel."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9dcaa",
   "metadata": {},
   "source": [
    "# converting these dataframe into csv files so that we can use them for training and testing and validation also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50e7cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.to_csv(\"./balanced_dataset_prepared/splits/train.csv\", index=None)\n",
    "test_df.to_csv(\"./balanced_dataset_prepared/splits/testing.csv\", index=None)\n",
    "val_df.to_csv(\"./balanced_dataset_prepared/splits/validation.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "586aa28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    747\n",
      "1    747\n",
      "Name: count, dtype: int64\n",
      "(1045, 2)\n",
      "(149, 2)\n",
      "(300, 2)\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df['Label'].value_counts())\n",
    "print(training_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208be235",
   "metadata": {},
   "source": [
    "# dataset loader for training and validation and testing df\n",
    "Note: okenized inputs must have the same length because deep learning models and DataLoaders operate on fixed-shape tensors, and padding enables batching, parallel computation, and efficient training without affecting model learning (via masking).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1f891c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "max_token = 0\n",
    "for i in training_df['Text']:\n",
    "    token_count = len(tokenizer.encode(i))\n",
    "    max_token = max(token_count, max_token)\n",
    "    # print(len(tokenizer.encode(i)) == 137)\n",
    "    \n",
    "print(max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65794e",
   "metadata": {},
   "source": [
    "# as you can see in our case in the trainig dataframe we have email of max_token length 137 , so our input batch size should be 137 tokens in each tokenized input sample for training , validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14887159",
   "metadata": {},
   "source": [
    "# note\n",
    "### Dataset:\n",
    "\n",
    "A Dataset defines how individual data samples are loaded and preprocessed, providing one sample at a time.\n",
    "\n",
    "### DataLoader:\n",
    "\n",
    "A DataLoader handles batching, shuffling, and parallel loading of data from a Dataset for efficient model training.# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8235a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff3e55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None,pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        self.encoded_texts = [ tokenizer.encode(text) for text in self.data['Text']]\n",
    "        # self.labels = self.data['label'].values\n",
    "\n",
    "\n",
    "        # /*****\n",
    "            #if encoded text is less than max_length then pad it with pad_token_id\n",
    "            #if encoded text is greater than max_length then truncate it\n",
    "        # *****/\n",
    "        if max_length is None:\n",
    "            self.max_length = self.__longest__encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            if(len(encoded_text) < self.max_length)else encoded_text[:self.max_length]\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.encoded_texts[idx]\n",
    "        label = self.data.iloc[idx]['Label']\n",
    "        return (\n",
    "            torch.tensor(encoded,dtype = torch.long),\n",
    "            torch.tensor(label,dtype = torch.long)\n",
    "        )\n",
    "      \n",
    "    def __longest__encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_text_len = len(encoded_text)\n",
    "            if(encoded_text_len > max_length):\n",
    "                max_length = encoded_text_len\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a414579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(csv_file = \"balanced_dataset_prepared/splits/train.csv\",max_length=None,tokenizer=tokenizer)\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1beb37db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "val_dataset = SpamDataset(csv_file = \"balanced_dataset_prepared/splits/validation.csv\",max_length=train_dataset.max_length,tokenizer=tokenizer)\n",
    "print(val_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee54114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = SpamDataset(csv_file = \"balanced_dataset_prepared/splits/testing.csv\",max_length=train_dataset.max_length,tokenizer=tokenizer)\n",
    "print(test_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e4c09",
   "metadata": {},
   "source": [
    "# DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef3d9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab5d8fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27032009610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566d85e",
   "metadata": {},
   "source": [
    "# DataLoader shuffles indices, splits them into batches, spawns workers(num_workers) to fetch samples, collates them, and yields batches one by one.\n",
    "Note Your datasetâ€™s __getitem__ and __len__ are the backbone of this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5b94f",
   "metadata": {},
   "source": [
    "- `encoded_text` is a padded sequence of token IDs (length = `max_length`)\n",
    "- `label` is a single integer\n",
    "\n",
    "---\n",
    "\n",
    "### Example Setup\n",
    "Letâ€™s say:\n",
    "- `batch_size = 3`\n",
    "- `max_length = 6`\n",
    "- `pad_token_id = 50256`\n",
    "\n",
    "And your dataset has these three samples:\n",
    "\n",
    "```text\n",
    "Sample 1: encoded_text = [12, 45, 78]              â†’ label = 0\n",
    "Sample 2: encoded_text = [34, 56, 90, 12, 77]      â†’ label = 1\n",
    "Sample 3: encoded_text = [99, 100, 101, 102, 103]  â†’ label = 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### After Padding/Truncation\n",
    "Each sequence is padded/truncated to length 6:\n",
    "\n",
    "```text\n",
    "Sample 1 â†’ [12, 45, 78, 50256, 50256, 50256]\n",
    "Sample 2 â†’ [34, 56, 90, 12, 77, 50256]\n",
    "Sample 3 â†’ [99, 100, 101, 102, 103, 50256]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Tensor from DataLoader\n",
    "When `DataLoader` collates them, you get:\n",
    "\n",
    "```python\n",
    "X_batch = tensor([\n",
    "    [   12,    45,    78, 50256, 50256, 50256],\n",
    "    [   34,    56,    90,    12,    77, 50256],\n",
    "    [   99,   100,   101,   102,   103, 50256]\n",
    "], dtype=torch.long)\n",
    "\n",
    "y_batch = tensor([0, 1, 0], dtype=torch.long)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Shapes\n",
    "- `X_batch.shape = (batch_size, max_length(columns bol skte he))` â†’ `(3, 6)`\n",
    "- `y_batch.shape = (batch_size,)` â†’ `(3,)`\n",
    "\n",
    "---\n",
    "\n",
    "So in practice, **each batch is a 2D tensor of token IDs plus a 1D tensor of labels**.  \n",
    "\n",
    "ðŸ‘‰When you make drop last = true , it will drop the last samples if they are less than the batch_size mtlb agar wo ek batch milkr nhi banapayenge to hum unhe discard kr denge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21b147d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    batch_size = batch_size,\n",
    "    num_workers = num_workers,\n",
    "    dataset = train_dataset,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa67ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    shuffle = True,\n",
    "    drop_last = False,\n",
    "    batch_size = batch_size,\n",
    "    num_workers= num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a602292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader =DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    num_workers= num_workers,\n",
    "    batch_size=  batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2417",
   "metadata": {},
   "source": [
    "# chekcing the dimensions of the loaders \n",
    "`ensuring that dimensions are consisitent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e08df",
   "metadata": {},
   "source": [
    "`train-loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "018a96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input and target batch : \n",
      " torch.Size([8, 137]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, taget_batch in train_loader:\n",
    "    pass\n",
    "print(\"shape of input and target batch : \\n\",input_batch.shape, taget_batch.shape) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731cae1f",
   "metadata": {},
   "source": [
    "`test_loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84696f",
   "metadata": {},
   "source": [
    "#### test dataset has 300 samples,  batch size is 8.\n",
    "```\n",
    "`300 \\div 8 = 37 full batches, with a remainder of 4 samples (37 *  8 = 296; 300 - 296 = 4$).`\n",
    "\n",
    "`this loop runs through all 38 batches. When the loop finishes, the variables input_batch and taget_batch hold the values from the very last iteration, which is the \"remainder\" batch containing the final 4 samples`\n",
    "\n",
    "`note : becoz we have set drop_last = false ` \n",
    "### that's why the we are getting the last batch of 4 samples `\n",
    "shape of input and target batch : \n",
    "input_batchtorch.Size([4, 137])\n",
    "torch.Size([4]) ->\n",
    "\n",
    "\n",
    " here  4 is the batch size-> `mtlb 4 samples honge batch me` and 137 is the max length of the input text (mtlb  137 tokens honge ek input sample me maxmimum)\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8eef3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b334326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input and target batch : \n",
      " torch.Size([4, 137]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, taget_batch in test_loader:\n",
    "    pass\n",
    "print(\"shape of input and target batch : \\n\",input_batch.shape, taget_batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e28e2",
   "metadata": {},
   "source": [
    "`val_loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bfb28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input batch torch.Size([4, 137]) and target batch :torch.Size([4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_batch, taget_batch in test_loader:\n",
    "    pass\n",
    "print(f\"shape of input batch {input_batch.shape} and target batch :{ taget_batch.shape} \\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af133924",
   "metadata": {},
   "source": [
    "-`length of these loaders shows number of batches in it same for other loader (val, test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cad6e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb4628",
   "metadata": {},
   "source": [
    "``This concludes the dataPreparation part of the project `` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984169c",
   "metadata": {},
   "source": [
    "# STAGE 2 - MODEL SET UP\n",
    "```\n",
    "1.INTIALIZE THE MODEL\n",
    "2.LOAD THE PRETRAINED WEIGHT\n",
    "3.FREEZE THE ALL EXCEPT LAST LAYERS(FINAL OUTPUT HEAD , FINAL TRANSFORMER BLOCK, FINAL, FINAL NORMALIZATION LAYER)\n",
    "4. MODIFY MODEL FOR FINETUNING\n",
    "5. IMPLEMENT THE EVALUATION UTILITY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f8f91",
   "metadata": {},
   "source": [
    "## INITIALIZING A MODEL WITH PRETRAINED WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85d59e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee047b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c179775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cbdef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a66257d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fecab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c80a771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small(124M)\"\n",
    "INPUT_PROMPT = \"EVERY EFFORT MOVES YOU IN  THE RIGHT DIRECTION\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "   \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "     \"gpt2-small(124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12}\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "print(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de434ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(BASE_CONFIG)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14683e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac01cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if(left.shape != right.shape):\n",
    "        raise(ValueError(f\"Shape mismatch : {left.shape} and right {right.shape}\"))\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b231c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ca967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\",models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb1676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc73ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0b1e7",
   "metadata": {},
   "source": [
    "| Component | Full Form                  | Role                   |\n",
    "| --------- | -------------------------- | ---------------------- |\n",
    "| `wte`     | Weight Token Embeddings    | Token â†’ vector mapping |\n",
    "| `wpe`     | Weight Position Embeddings | Injects order info     |\n",
    "| `blocks`  | Transformer blocks         | Core computation       |\n",
    "| `g`       | Gain (Î³)                   | LayerNorm scaling      |\n",
    "| `b`       | Bias (Î²)                   | LayerNorm shifting     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71abb6b",
   "metadata": {},
   "source": [
    "b and g â€” Bias and Gain (LayerNorm parameters)\n",
    "\n",
    "These usually appear inside LayerNorms.\n",
    "Where they appear\n",
    "\n",
    "ln_1 (before attention)\n",
    "\n",
    "ln_2 (before MLP)\n",
    "\n",
    "Final layer norm (ln_f)\n",
    "\n",
    "Formula of LayerNorm\n",
    "LN(x)=gâ‹…(xâˆ’Î¼)/Ïƒâ€‹+b\n",
    "\n",
    "| Symbol  | Name     | Shape       | Role                     |\n",
    "| ------- | -------- | ----------- | ------------------------ |\n",
    "| **`g`** | Gain (Î³) | `[emb_dim]` | Scales normalized values |\n",
    "| **`b`** | Bias (Î²) | `[emb_dim]` | Shifts normalized values |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d3720",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "By default, the GPTModel instance is initialized with random weights for pretraining. \n",
    "\n",
    "The last\n",
    "step to using OpenAI's model weights is to override these random weights with the weights\n",
    "we loaded into the params dictionary.\n",
    "\n",
    "For this, we will first define a small assign utility function that checks whether two\n",
    "tensors or arrays (left and right) have the same dimensions or shape and returns the\n",
    "right tensor as trainable PyTorch parameters:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2be24",
   "metadata": {},
   "source": [
    "we need to check whether the loading parameters are of proper dimensions before laoding them into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fc6fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(BASE_CONFIG)\n",
    "# gpt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc147141",
   "metadata": {},
   "source": [
    "now lets try to load the weights into gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt,params)\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfd0b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60bfea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def text_to_tokens(text, tokenizer):\n",
    "    encoded= tokenizer.encode(text, allowed_special= 'all')\n",
    "    token_ids = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa11a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =(  \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = generate_text_simple(\n",
    "    model = gpt,\n",
    "    idx = text_to_tokens(prompt, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d37ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or\n"
     ]
    }
   ],
   "source": [
    "print(token_ids_to_text(text, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533c001",
   "metadata": {},
   "source": [
    "## 3. freeze the model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gpt.parameters():\n",
    "    param.requires_grad = False #by setting false we are making all weights untrainable and frozen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fca8db",
   "metadata": {},
   "source": [
    "### changin the out_head to map the input layer to the number of classed \n",
    "the classes are 2 (spam and not spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98e5c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "num_classes = 2\n",
    "gpt.out_head = torch.nn.Linear(in_features = BASE_CONFIG[\"emb_dim\"], out_features = num_classes) # it means it willl map 768 dim vector into 2 dim vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e5ba8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that in the preceding code, we use BASE_CONFIG[\"emb_dim\"], which is equal to 768 in\n",
    "the \"gpt2-small (124M)\" model, to keep the code below more general. \n",
    "\n",
    "This means we\n",
    "can also use the same code to work with the larger GPT-2 model variants.\n",
    "\n",
    "This new model.out_head output layer has its requires_grad attribute set to True by\n",
    "default, which means that it's the only layer in the model that will be updated during\n",
    "training.\n",
    "\n",
    "\n",
    "\n",
    "Stage 1: Train only out_head\n",
    "Stage 2: Train out_head + last block + ln_f\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7ca56",
   "metadata": {},
   "source": [
    "Input Text\n",
    " â†’ Tokenization\n",
    " â†’ Token Embeddings (wte)\n",
    " â†’ Position Embeddings (wpe)\n",
    " â†’ Transformer Blocks (Ã— n_layers)\n",
    " â†’ Final LayerNorm (ln_f)\n",
    " â†’ Output Head (out_head)\n",
    " â†’ Logits\n",
    " â†’ Loss\n",
    " â†’ Backprop (only selected layers update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ec7b0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making final transofrmer block \n",
    "for param in gpt.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# making final_norm layer trainaible\n",
    "for param in gpt.final_norm.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a34a687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   703,   389,   345,    30]])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.encode(\"Hello, how are you?\")\n",
    "inputs = torch.tensor(prompt).unsqueeze(0)\n",
    "print(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c5e84e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 2])\n",
      "tensor([[[1.6807, 1.6095],\n",
      "         [3.7947, 8.1032],\n",
      "         [3.3092, 8.4428],\n",
      "         [2.0253, 6.9588],\n",
      "         [2.2015, 7.5989],\n",
      "         [3.6070, 7.6458]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = gpt(inputs)\n",
    "print(out.shape)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095d1cd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If we had used the original GPT-2 model, a similar input would have produced an output tensor of [1, 6, 50257],\n",
    "where 50,257 represents the vocabulary size. \n",
    "\n",
    "As we are using a smaller model (124M), the number of\n",
    "output rows corresponds to the number of input tokens (in this case, 6). \n",
    "\n",
    "However, each\n",
    "output's embedding dimension (the number of columns) is now reduced to 2 instead of\n",
    "50,257 since we replaced the output layer of the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "96be8171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6070, 7.6458]])\n"
     ]
    }
   ],
   "source": [
    "#we need to last output token to get the classification becaue it the last token that contain the information of the other tokens\n",
    "print(out[:,-1,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d26dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Having modified the model, the next section will detail the process of transforming the\n",
    "last token into class label predictions and calculate the model's initial prediction accuracy.\n",
    "\n",
    "Following this, we will finetune the model for the spam classification task in the subsequent\n",
    "section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4713446",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can obtain the class label via the following code:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42c1fd",
   "metadata": {},
   "source": [
    "we have obtained the output logits for the input data now we want the classification results , we can use torch.softmax then argmax to get the index of the maximum value in the logits array but to simplfiy the computaiton using argmax will also work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fdaa5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0173, 0.9827]])\n",
      "class label:  1\n"
     ]
    }
   ],
   "source": [
    "# option1 \n",
    "prob = torch.softmax(out[:,-1,:], dim=-1)\n",
    "print(prob)\n",
    "\n",
    "label = torch.argmax(prob)\n",
    "print(\"class label: \",label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098139ab",
   "metadata": {},
   "source": [
    "`torch.argmax: This converts raw probability scores (logits) into specific class predictions by finding the index of the highest score.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73204f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "class label:  1\n"
     ]
    }
   ],
   "source": [
    "# option 2 (recommended)\n",
    "\n",
    "logits = torch.argmax(out[:,-1,:])\n",
    "print(logits)\n",
    "print(\"class label: \",logits.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306d2bf",
   "metadata": {},
   "source": [
    "`her you can see that the class lable is 1 in t both ways `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32937f91",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To determine the classification accuracy, we apply the argmax-based prediction code to\n",
    "all examples in the dataset and calculate the proportion of correct predictions by defining a\n",
    "calc_accuracy_loader function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "49cf8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    #num_batches is nothing but the number of batches wich we want to process\n",
    "    model.eval() # it will ensure that the model is in evaluation mode and all neurons are active \n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) # it will take the minimum number for batches its only for due to computation Safety: The min() function ensures code doesn't crash if you ask for more batches than exist in the loader.\n",
    "    \n",
    "\n",
    "    for i , (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches: #by using i we are doing that caclulating only upto num_batches specified , for enumerate was required becoz it can convert input_batch, target_batch pair in index based.\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad(): # by using no_grad we are stopping gradient weights to be calcualated it will save computation time becz for testing we dont need it\n",
    "                logits = model(input_batch)[:,-1,:] #it will get the last output token of every input tensor returned by model\n",
    "                \n",
    "            predicted_labels = torch.argmax(logits,dim = -1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return correct_predictions/num_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c2b39",
   "metadata": {},
   "source": [
    "``Detaching from the Graph: PyTorch tensors often carry a \"computational history\" (the gradient graph). If you add a tensor to a running sum without .item(), you are effectively keeping the entire history of every batch in memory.\n",
    "Preventing Memory Leaks: Without .item(), you may encounter an \"Out of Memory\" (OOM) error because the memory allocated for the training graph cannot be freed.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b65f1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0f38371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from Architecture Classification finetuning import train_loader\n",
    "gpt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "13271275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Architecture Classification finetuning import val_loader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, num_batches = 10, device= device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, num_batches = 10, device= device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, num_batches = 10, device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "35b9909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.00%\n",
      "51.25%\n",
      "45.00%\n"
     ]
    }
   ],
   "source": [
    "# from Architecture Classification finetuning import val_accuracy\n",
    "# from Architecture Classification finetuning import test_accuracy\n",
    "print(f\"{train_accuracy*100:.2f}%\")\n",
    "print(f\"{val_accuracy*100:.2f}%\")\n",
    "print(f\"{test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd6125",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the prediction accuracies are near a random prediction, which would be\n",
    "50% in this case. \n",
    "\n",
    "To improve the prediction accuracies, we need to finetune the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? False\n",
      "Current Device Count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "print(f\"Current Device Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190b707",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Classification accuracy is not a differentiable function, so we use cross entropy\n",
    "loss as a proxy to maximize accuracy. \n",
    "\n",
    "This is the same cross entropy loss discussed earlier. \n",
    "\n",
    "Accordingly, the calc_loss_batch function remains the same as in earlier, with one\n",
    "adjustment: we focus on optimizing only the last token, model(input_batch)[:, -1, :],\n",
    "rather than all tokens, model(input_batch):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "626ba5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device) , target_batch.to(device)\n",
    "    logits = model(input_batch)[:,-1,:]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb413a",
   "metadata": {},
   "source": [
    "# we use calc_loss_batch to calcualte loss for single batch obtained from defined dataloaders. To calculate loss for all batches in a dataloader, we define the `calc_loss_loader function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ec853c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches= None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('Nan')\n",
    "    elif num_batches == None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # if num_batches exceeds the number of batches in datloader then we need to set this:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i , (input_batch , target_batch) in enumerate(data_loader):\n",
    "        if(i < num_batches):\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "        return total_loss/num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e91da",
   "metadata": {},
   "source": [
    "similarly like we have calculated the accuracy for train_loader . We now compute the initial oss for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a108911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Architecture Classification finetuning import test_loader\n",
    "with torch.no_grad(): #disable gradient tracking for efficiency becz we are not training yet\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches= 5)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches= 5)\n",
    "    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d1092f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.229\n",
      "Validation loss: 0.193\n",
      "Test loss: 15.765\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {100*test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d79af",
   "metadata": {},
   "source": [
    "# finetuing model on supervised data , implementing training\n",
    "In the next section, we will implement a training function to finetune the model, which\n",
    "means adjusting the model to minimize the training set loss. \n",
    "\n",
    "Minimizing the training set\n",
    "loss will help increase the classification accuracy, our overall goa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
